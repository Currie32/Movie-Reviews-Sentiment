{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the Sentiment of Movie Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two goals for this analysis. The first is to accurately predict the sentiment of movie reviews, and the second is to develop my model in such a way that its outputs can be analyzed with TensorBoard. This is the first time that I am using TensorBoard, so I want to have a somewhat challenging task, and not use a huge dataset. There are 25,000 training and testing reviews, so this model can train multiple iterations overnight on my MacBook Pro. The data is provided by a Kaggle competition from 2015 (https://www.kaggle.com/c/word2vec-nlp-tutorial). Despite it having concluded, it can still be used as an excellent learning opportunity. The sections of this analysis are:\n",
    "- Inspect the Data\n",
    "- Clean and Format the Data\n",
    "- Build and Train the Model\n",
    "- Make the Predictions\n",
    "- Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import nltk, re, time\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "train = pd.read_csv(\"labeledTrainData.tsv\", delimiter=\"\\t\")\n",
    "test = pd.read_csv(\"testData.tsv\", delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7759_3</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3630_4</td>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9495_8</td>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  sentiment                                             review\n",
       "0  5814_8          1  With all this stuff going down at the moment w...\n",
       "1  2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
       "2  7759_3          0  The film starts with a manager (Nicholas Bell)...\n",
       "3  3630_4          0  It must be assumed that those who praised this...\n",
       "4  9495_8          1  Superbly trashy and wondrously unpretentious 8..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12311_10</td>\n",
       "      <td>Naturally in a film who's main themes are of m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8348_2</td>\n",
       "      <td>This movie is a disaster within a disaster fil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5828_4</td>\n",
       "      <td>All in all, this is a movie for kids. We saw i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7186_2</td>\n",
       "      <td>Afraid of the Dark left me with the impression...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12128_7</td>\n",
       "      <td>A very accurate depiction of small time mob li...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                             review\n",
       "0  12311_10  Naturally in a film who's main themes are of m...\n",
       "1    8348_2  This movie is a disaster within a disaster fil...\n",
       "2    5828_4  All in all, this is a movie for kids. We saw i...\n",
       "3    7186_2  Afraid of the Dark left me with the impression...\n",
       "4   12128_7  A very accurate depiction of small time mob li..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 3)\n",
      "(25000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reviews are rather long, so we won't be using all of the text to train our model. Using all of the text would increase our training to a longer timeframe than I would rather give to this project, but it should make the predictions more accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With all this stuff going down at the moment with MJ i've started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ's feeling towards the press and also the obvious message of drugs are bad m'kay.<br /><br />Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring. Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him.<br /><br />The actual feature film bit when it finally starts is only on for 20 minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord. Why he wants MJ dead so bad is beyond me. Because MJ overheard his plans? Nah, Joe Pesci's character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno, maybe he just hates MJ's music.<br /><br />Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence. Also, the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene.<br /><br />Bottom line, this movie is for people who like MJ on one level or another (which i think is most people). If not, then stay away. It does try and give off a wholesome message and ironically MJ's bestest buddy in this movie is a girl! Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty? Well, with all the attention i've gave this subject....hmmm well i don't know because people can be different behind closed doors, i know this for a fact. He is either an extremely nice but stupid guy or one of the most sickest liars. I hope he is not the latter.\n",
      "\n",
      "\\The Classic War of the Worlds\\\" by Timothy Hines is a very entertaining film that obviously goes to great effort and lengths to faithfully recreate H. G. Wells' classic book. Mr. Hines succeeds in doing so. I, and those who watched his film with me, appreciated the fact that it was not the standard, predictable Hollywood fare that comes out every year, e.g. the Spielberg version with Tom Cruise that had only the slightest resemblance to the book. Obviously, everyone looks for different things in a movie. Those who envision themselves as amateur \\\"critics\\\" look only to criticize everything they can. Others rate a movie on more important bases,like being entertained, which is why most people never agree with the \\\"critics\\\". We enjoyed the effort Mr. Hines put into being faithful to H.G. Wells' classic novel, and we found it to be very entertaining. This made it easy to overlook what the \\\"critics\\\" perceive to be its shortcomings.\"\n",
      "\n",
      "The film starts with a manager (Nicholas Bell) giving welcome investors (Robert Carradine) to Primal Park . A secret project mutating a primal animal using fossilized DNA, like ¨Jurassik Park¨, and some scientists resurrect one of nature's most fearsome predators, the Sabretooth tiger or Smilodon . Scientific ambition turns deadly, however, and when the high voltage fence is opened the creature escape and begins savagely stalking its prey - the human visitors , tourists and scientific.Meanwhile some youngsters enter in the restricted area of the security center and are attacked by a pack of large pre-historical animals which are deadlier and bigger . In addition , a security agent (Stacy Haiduk) and her mate (Brian Wimmer) fight hardly against the carnivorous Smilodons. The Sabretooths, themselves , of course, are the real star stars and they are astounding terrifyingly though not convincing. The giant animals savagely are stalking its prey and the group run afoul and fight against one nature's most fearsome predators. Furthermore a third Sabretooth more dangerous and slow stalks its victims.<br /><br />The movie delivers the goods with lots of blood and gore as beheading, hair-raising chills,full of scares when the Sabretooths appear with mediocre special effects.The story provides exciting and stirring entertainment but it results to be quite boring .The giant animals are majority made by computer generator and seem totally lousy .Middling performances though the players reacting appropriately to becoming food.Actors give vigorously physical performances dodging the beasts ,running,bound and leaps or dangling over walls . And it packs a ridiculous final deadly scene. No for small kids by realistic,gory and violent attack scenes . Other films about Sabretooths or Smilodon are the following : ¨Sabretooth(2002)¨by James R Hickox with Vanessa Angel, David Keith and John Rhys Davies and the much better ¨10.000 BC(2006)¨ by Roland Emmerich with with Steven Strait, Cliff Curtis and Camilla Belle. This motion picture filled with bloody moments is badly directed by George Miller and with no originality because takes too many elements from previous films. Miller is an Australian director usually working for television (Tidal wave, Journey to the center of the earth, and many others) and occasionally for cinema ( The man from Snowy river, Zeus and Roxanne,Robinson Crusoe ). Rating : Below average, bottom of barrel.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect the reviews\n",
    "for i in range(3):\n",
    "    print(train.review[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id           0\n",
      "sentiment    0\n",
      "review       0\n",
      "dtype: int64\n",
      "id        0\n",
      "review    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for any null values\n",
    "print(train.isnull().sum())\n",
    "print(test.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean and Format the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text(text, remove_stopwords=True):\n",
    "    '''Clean the text, with the option to remove stopwords'''\n",
    "    \n",
    "    # Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "\n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"<br />\", \" \", text)\n",
    "    text = re.sub(r\"[^a-z]\", \" \", text)\n",
    "    text = re.sub(r\"   \", \" \", text) # Remove any extra spaces\n",
    "    text = re.sub(r\"  \", \" \", text)\n",
    "    \n",
    "    # Remove punctuation from text\n",
    "    text = ''.join([c for c in text if c not in punctuation])\n",
    "    \n",
    "    # Return a list of words\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the training and testing reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_clean = []\n",
    "for review in train.review:\n",
    "    train_clean.append(clean_text(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_clean = []\n",
    "for review in test.review:\n",
    "    test_clean.append(clean_text(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stuff going moment mj i ve started listening music watching odd documentary there watched wiz watched moonwalker again maybe want get certain insight guy thought really cool eighties maybe make mind whether guilty innocent moonwalker part biography part feature film remember going see cinema originally released subtle messages mj s feeling towards press also obvious message drugs bad m kay visually impressive course michael jackson unless remotely like mj anyway going hate find boring may call mj egotist consenting making movie mj fans would say made fans true really nice him the actual feature film bit finally starts minutes excluding smooth criminal sequence joe pesci convincing psychopathic powerful drug lord wants mj dead bad beyond me mj overheard plans nah joe pesci s character ranted wanted people know supplying drugs etc dunno maybe hates mj s music lots cool things like mj turning car robot whole speed demon sequence also director must patience saint came filming kiddy bad sequence usually directors hate working one kid let alone whole bunch performing complex dance scene bottom line movie people like mj one level another which think people not stay away try give wholesome message ironically mj s bestest buddy movie girl michael jackson truly one talented people ever grace planet guilty well attention i ve gave subject hmmm well don t know people different behind closed doors know fact either extremely nice stupid guy one sickest liars hope latter \n",
      "\n",
      " the classic war worlds timothy hines entertaining film obviously goes great effort lengths faithfully recreate h g wells classic book mr hines succeeds so i watched film me appreciated fact standard predictable hollywood fare comes every year e g spielberg version tom cruise slightest resemblance book obviously everyone looks different things movie envision amateur critics look criticize everything can others rate movie important bases like entertained people never agree critics enjoyed effort mr hines put faithful h g wells classic novel found entertaining made easy overlook critics perceive shortcomings \n",
      "\n",
      "film starts manager nicholas bell giving welcome investors robert carradine primal park secret project mutating primal animal using fossilized dna like jurassik park scientists resurrect one nature s fearsome predators sabretooth tiger smilodon scientific ambition turns deadly however high voltage fence opened creature escape begins savagely stalking prey human visitors tourists scientific meanwhile youngsters enter restricted area security center attacked pack large pre historical animals deadlier bigger addition security agent stacy haiduk mate brian wimmer fight hardly carnivorous smilodons sabretooths course real star stars astounding terrifyingly though convincing giant animals savagely stalking prey group run afoul fight one nature s fearsome predators furthermore third sabretooth dangerous slow stalks victims the movie delivers goods lots blood gore beheading hair raising chills full scares sabretooths appear mediocre special effects the story provides exciting stirring entertainment results quite boring the giant animals majority made computer generator seem totally lousy middling performances though players reacting appropriately becoming food actors give vigorously physical performances dodging beasts running bound leaps dangling walls packs ridiculous final deadly scene small kids realistic gory violent attack scenes films sabretooths smilodon following sabretooth  by james r hickox vanessa angel david keith john rhys davies much better  bc  roland emmerich steven strait cliff curtis camilla belle motion picture filled bloody moments badly directed george miller originality takes many elements previous films miller australian director usually working television tidal wave journey center earth many others occasionally cinema man snowy river zeus roxanne robinson crusoe rating average bottom barrel \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect the cleaned reviews\n",
    "for i in range(3):\n",
    "    print(train_clean[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting is complete.\n",
      "train_seq is complete.\n",
      "test_seq is complete\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the reviews\n",
    "all_reviews = train_clean + test_clean\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_reviews)\n",
    "print(\"Fitting is complete.\")\n",
    "\n",
    "train_seq = tokenizer.texts_to_sequences(train_clean)\n",
    "print(\"train_seq is complete.\")\n",
    "\n",
    "test_seq = tokenizer.texts_to_sequences(test_clean)\n",
    "print(\"test_seq is complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in index: 99426\n"
     ]
    }
   ],
   "source": [
    "# Find the number of unique tokens\n",
    "word_index = tokenizer.word_index\n",
    "print(\"Words in index: %d\" % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[445, 86, 489, 10939, 8, 61, 583, 2603, 120, 68, 957, 560, 53, 212, 24485, 212, 17247, 219, 193, 97, 20, 695, 2565, 124, 109, 15, 520, 3954, 193, 27, 246, 654, 2352, 1261, 17247, 90, 4782, 90, 712, 3, 305, 86, 16, 358, 1846, 542, 1219, 3592, 10939, 1, 485, 871, 3538, 23, 526, 673, 1414, 19, 63, 5305, 2089, 1118, 185, 413, 1523, 817, 2583, 7, 10939, 477, 86, 665, 85, 272, 114, 578, 10939, 34480, 29662, 148, 2, 10939, 381, 13, 59, 26, 381, 210, 15, 252, 178, 10, 751, 712, 3, 142, 341, 464, 145, 16427, 4121, 1718, 635, 876, 10547, 1018, 12089, 890, 1067, 1652, 416, 10939, 265, 19, 596, 141, 10939, 18336, 2302, 15821, 876, 10547, 1, 34, 38190, 388, 21, 49, 17539, 1414, 434, 9821, 193, 4238, 10939, 1, 120, 669, 520, 96, 7, 10939, 1555, 444, 2271, 138, 2137, 2383, 635, 23, 72, 117, 4750, 5364, 307, 1326, 31136, 19, 635, 556, 888, 665, 697, 6, 452, 195, 547, 138, 689, 3386, 1234, 790, 56, 1239, 268, 2, 21, 7, 10939, 6, 580, 78, 476, 32, 21, 245, 706, 158, 276, 113, 7674, 673, 3526, 10939, 1, 37925, 1690, 2, 159, 413, 1523, 294, 6, 956, 21, 51, 1500, 1226, 2352, 17, 612, 8, 61, 442, 724, 7184, 17, 25, 4, 49, 21, 199, 443, 3912, 3484, 49, 110, 270, 495, 252, 289, 124, 6, 19622, 19910, 363, 1502]\n",
      "\n",
      "[10, 280, 218, 3564, 4519, 10533, 355, 3, 460, 188, 22, 708, 8493, 11052, 8659, 1863, 1090, 4622, 280, 189, 366, 10533, 2753, 182, 8, 212, 3, 141, 2526, 110, 1200, 648, 269, 2562, 192, 89, 214, 680, 1090, 3331, 238, 757, 3737, 3888, 4331, 189, 460, 215, 208, 199, 96, 2, 20036, 2381, 1347, 84, 6489, 186, 92, 324, 895, 2, 576, 12500, 7, 2255, 21, 43, 950, 1347, 439, 708, 366, 10533, 191, 2792, 1863, 1090, 4622, 280, 600, 173, 355, 26, 691, 5240, 1347, 11140, 5636]\n",
      "\n",
      "[3, 464, 2887, 4790, 3631, 668, 2492, 17093, 537, 4494, 10280, 1157, 937, 1062, 32772, 10280, 1642, 718, 10347, 7, 1157, 3035, 11433, 6, 836, 1, 18155, 9619, 14817, 5059, 47340, 3636, 5360, 440, 2429, 107, 220, 27045, 7174, 2944, 1481, 972, 762, 17403, 6324, 4734, 315, 9375, 8934, 3636, 1946, 7468, 2265, 11236, 1537, 2500, 2101, 2943, 3032, 958, 1723, 1252, 1524, 35849, 1862, 1606, 2500, 1406, 9610, 39014, 4034, 1698, 484, 962, 24190, 33320, 185, 66, 243, 327, 5692, 20772, 76, 1018, 1265, 1524, 17403, 6324, 4734, 471, 420, 16656, 484, 6, 836, 1, 18155, 9619, 3803, 802, 14817, 1673, 488, 9649, 1462, 10, 2, 1477, 6281, 669, 473, 529, 17060, 1030, 5075, 5648, 286, 2682, 33320, 906, 1507, 228, 207, 10, 14, 1536, 1073, 8078, 649, 1950, 98, 272, 10, 1265, 1524, 2190, 26, 1223, 20227, 232, 384, 2215, 22311, 292, 76, 1814, 12566, 5112, 1544, 1610, 69, 113, 20461, 1658, 292, 16694, 12750, 563, 2701, 9210, 13008, 3627, 6981, 573, 395, 2429, 56, 323, 271, 733, 2007, 1055, 1285, 60, 35, 33320, 47340, 898, 14817, 968, 532, 1177, 32498, 6549, 2204, 521, 4035, 229, 8246, 3860, 18, 54, 16526, 9924, 36398, 2039, 16832, 4008, 4107, 8164, 6529, 1227, 360, 992, 1640, 311, 856, 450, 655, 2664, 2700, 227, 39, 741, 858, 35, 2664, 2053, 72, 556, 697, 617, 23140, 3124, 1180, 2101, 620, 39, 324, 1860, 358, 52, 9282, 1842, 18753, 28190, 4353, 598, 775, 1239, 5295]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect the reviews after they have been tokenized\n",
    "for i in range(3):\n",
    "    print(train_seq[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find the length of reviews\n",
    "lengths = []\n",
    "for review in train_seq:\n",
    "    lengths.append(len(review))\n",
    "\n",
    "for review in test_seq:\n",
    "    lengths.append(len(review))\n",
    "\n",
    "# Create a dataframe so that the values can be inspected\n",
    "lengths = pd.DataFrame(lengths, columns=['counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    50000.000000\n",
       "mean       131.048860\n",
       "std         98.409141\n",
       "min          3.000000\n",
       "25%         70.000000\n",
       "50%         98.000000\n",
       "75%        160.000000\n",
       "max       1476.000000\n",
       "Name: counts, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths.counts.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182.0\n",
      "213.0\n",
      "258.0\n",
      "338.0\n"
     ]
    }
   ],
   "source": [
    "print(np.percentile(lengths.counts, 80))\n",
    "print(np.percentile(lengths.counts, 85))\n",
    "print(np.percentile(lengths.counts, 90))\n",
    "print(np.percentile(lengths.counts, 95))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to use 200 as the maximum length of a review. Although longer would be better, I want to limit the training time, and this value will still provide the full text to more than 80% of the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_pad is complete.\n",
      "test_pad is complete.\n"
     ]
    }
   ],
   "source": [
    "# Pad and truncate the questions so that they all have the same length.\n",
    "max_review_length = 200\n",
    "\n",
    "train_pad = pad_sequences(train_seq, maxlen = max_review_length)\n",
    "print(\"train_pad is complete.\")\n",
    "\n",
    "test_pad = pad_sequences(test_seq, maxlen = max_review_length)\n",
    "print(\"test_pad is complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   90   712     3   305    86    16   358  1846   542  1219  3592 10939\n",
      "     1   485   871  3538    23   526   673  1414    19    63  5305  2089\n",
      "  1118   185   413  1523   817  2583     7 10939   477    86   665    85\n",
      "   272   114   578 10939 34480 29662   148     2 10939   381    13    59\n",
      "    26   381   210    15   252   178    10   751   712     3   142   341\n",
      "   464   145 16427  4121  1718   635   876 10547  1018 12089   890  1067\n",
      "  1652   416 10939   265    19   596   141 10939 18336  2302 15821   876\n",
      " 10547     1    34 38190   388    21    49 17539  1414   434  9821   193\n",
      "  4238 10939     1   120]\n",
      "\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "[  972   762 17403  6324  4734   315  9375  8934  3636  1946  7468  2265\n",
      " 11236  1537  2500  2101  2943  3032   958  1723  1252  1524 35849  1862\n",
      "  1606  2500  1406  9610 39014  4034  1698   484   962 24190 33320   185\n",
      "    66   243   327  5692 20772    76  1018  1265  1524 17403  6324  4734\n",
      "   471   420 16656   484     6   836     1 18155  9619  3803   802 14817\n",
      "  1673   488  9649  1462    10     2  1477  6281   669   473   529 17060\n",
      "  1030  5075  5648   286  2682 33320   906  1507   228   207    10    14\n",
      "  1536  1073  8078   649  1950    98   272    10  1265  1524  2190    26\n",
      "  1223 20227   232   384]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect the reviews after padding has been completed. \n",
    "for i in range(3):\n",
    "    print(train_pad[i,:100])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creating the training and validation sets\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(train_pad, train.sentiment, test_size = 0.15, random_state = 2)\n",
    "x_test = test_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21250, 200)\n",
      "(3750, 200)\n",
      "(25000, 200)\n"
     ]
    }
   ],
   "source": [
    "# Inspect the shape of the data\n",
    "print(x_train.shape)\n",
    "print(x_valid.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(x, y, batch_size):\n",
    "    '''Create the batches for the training and validation data'''\n",
    "    n_batches = len(x)//batch_size\n",
    "    x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        yield x[ii:ii+batch_size], y[ii:ii+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_test_batches(x, batch_size):\n",
    "    '''Create the batches for the testing data'''\n",
    "    n_batches = len(x)//batch_size\n",
    "    x = x[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        yield x[ii:ii+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_rnn(n_words, embed_size, batch_size, lstm_size, num_layers, \n",
    "              dropout, learning_rate, multiple_fc, fc_units):\n",
    "    '''Build the Recurrent Neural Network'''\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # Declare placeholders we'll feed into the graph\n",
    "    with tf.name_scope('inputs'):\n",
    "        inputs = tf.placeholder(tf.int32, [None, None], name='inputs')\n",
    "\n",
    "    with tf.name_scope('labels'):\n",
    "        labels = tf.placeholder(tf.int32, [None, None], name='labels')\n",
    "\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "    # Create the embeddings\n",
    "    with tf.name_scope(\"embeddings\"):\n",
    "        embedding = tf.Variable(tf.random_uniform((n_words, embed_size), -1, 1))\n",
    "        embed = tf.nn.embedding_lookup(embedding, inputs)\n",
    "\n",
    "    # Build the RNN layers\n",
    "    with tf.name_scope(\"RNN_layers\"):\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "        cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
    "    \n",
    "    # Set the initial state\n",
    "    with tf.name_scope(\"RNN_init_state\"):\n",
    "        initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    # Run the data through the RNN layers\n",
    "    with tf.name_scope(\"RNN_forward\"):\n",
    "        outputs, final_state = tf.nn.dynamic_rnn(cell, embed,\n",
    "                                                 initial_state=initial_state)    \n",
    "    \n",
    "    # Create the fully connected layers\n",
    "    with tf.name_scope(\"fully_connected\"):\n",
    "        \n",
    "        # Initialize the weights and biases\n",
    "        weights = tf.truncated_normal_initializer(stddev=0.1)\n",
    "        biases = tf.zeros_initializer()\n",
    "        \n",
    "        dense = tf.contrib.layers.fully_connected(outputs[:, -1],\n",
    "                                                  num_outputs = fc_units,\n",
    "                                                  activation_fn = tf.sigmoid,\n",
    "                                                  weights_initializer = weights,\n",
    "                                                  biases_initializer = biases)\n",
    "        dense = tf.contrib.layers.dropout(dense, keep_prob)\n",
    "        \n",
    "        # Depending on the iteration, use a second fully connected layer\n",
    "        if multiple_fc == True:\n",
    "            dense = tf.contrib.layers.fully_connected(dense,\n",
    "                                                      num_outputs = fc_units,\n",
    "                                                      activation_fn = tf.sigmoid,\n",
    "                                                      weights_initializer = weights,\n",
    "                                                      biases_initializer = biases)\n",
    "            dense = tf.contrib.layers.dropout(dense, keep_prob)\n",
    "    \n",
    "    # Make the predictions\n",
    "    with tf.name_scope('predictions'):\n",
    "        predictions = tf.contrib.layers.fully_connected(dense, \n",
    "                                                        num_outputs = 1, \n",
    "                                                        activation_fn=tf.sigmoid,\n",
    "                                                        weights_initializer = weights,\n",
    "                                                        biases_initializer = biases)\n",
    "        tf.summary.histogram('predictions', predictions)\n",
    "    \n",
    "    # Calculate the cost\n",
    "    with tf.name_scope('cost'):\n",
    "        cost = tf.losses.mean_squared_error(labels, predictions)\n",
    "        tf.summary.scalar('cost', cost)\n",
    "    \n",
    "    # Train the model\n",
    "    with tf.name_scope('train'):    \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "    # Determine the accuracy\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        correct_pred = tf.equal(tf.cast(tf.round(predictions), tf.int32), labels)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "        tf.summary.scalar('accuracy', accuracy)\n",
    "    \n",
    "    # Merge all of the summaries\n",
    "    merged = tf.summary.merge_all()    \n",
    "\n",
    "    # Export the nodes \n",
    "    export_nodes = ['inputs', 'labels', 'keep_prob', 'initial_state', 'final_state','accuracy',\n",
    "                    'predictions', 'cost', 'optimizer', 'merged']\n",
    "    Graph = namedtuple('Graph', export_nodes)\n",
    "    local_dict = locals()\n",
    "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get the following error 'ValueError: Dimensions must be equal, but are XXX and XXX for 'RNN_forward/rnn/while/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/MatMul_1' during training use this instead : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " def get_a_cell(lstm_size, keep_prob):\n",
    "            lstm = tf.nn.rnn_cell.BasicLSTMCell(lstm_size)\n",
    "            drop = tf.nn.rnn_cell.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "            return drop\n",
    "    \n",
    "    \n",
    "def build_rnn(n_words, embed_size, batch_size, lstm_size, num_layers, \n",
    "              dropout, learning_rate, multiple_fc, fc_units):\n",
    "    '''Build the Recurrent Neural Network'''\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # Declare placeholders we'll feed into the graph\n",
    "    with tf.name_scope('inputs'):\n",
    "        inputs = tf.placeholder(tf.int32, [None, None], name='inputs')\n",
    "\n",
    "    with tf.name_scope('labels'):\n",
    "        labels = tf.placeholder(tf.int32, [None, None], name='labels')\n",
    "\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "    # Create the embeddings\n",
    "    with tf.name_scope('embeddings'):\n",
    "        embedding = tf.Variable(tf.random_uniform((n_words, embed_size), -1, 1))\n",
    "        embed = tf.nn.embedding_lookup(embedding, inputs)\n",
    "\n",
    "    # Build the RNN layers\n",
    "    with tf.name_scope('lstm'):\n",
    "        cell = tf.nn.rnn_cell.MultiRNNCell([get_a_cell(lstm_size, keep_prob) for _ in range(num_layers)])\n",
    "    # Set the initial state\n",
    "    with tf.name_scope('RNN_init_state'):\n",
    "        initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    # Run the data through the RNN layers\n",
    "    with tf.name_scope('RNN_forward'):\n",
    "        outputs, final_state = tf.nn.dynamic_rnn(cell, embed,\n",
    "                                                 initial_state=initial_state)    \n",
    "    \n",
    "    # Create the fully connected layers\n",
    "    with tf.name_scope('fully_connected'):\n",
    "        \n",
    "        # Initialize the weights and biases\n",
    "        weights = tf.truncated_normal_initializer(stddev=0.1)\n",
    "        biases = tf.zeros_initializer()\n",
    "        \n",
    "        dense = tf.contrib.layers.fully_connected(outputs[:, -1],\n",
    "                                                  num_outputs = fc_units,\n",
    "                                                  activation_fn = tf.sigmoid,\n",
    "                                                  weights_initializer = weights,\n",
    "                                                  biases_initializer = biases)\n",
    "        dense = tf.contrib.layers.dropout(dense, keep_prob)\n",
    "        \n",
    "        # Depending on the iteration, use a second fully connected layer\n",
    "        if multiple_fc == True:\n",
    "            dense = tf.contrib.layers.fully_connected(dense,\n",
    "                                                      num_outputs = fc_units,\n",
    "                                                      activation_fn = tf.sigmoid,\n",
    "                                                      weights_initializer = weights,\n",
    "                                                      biases_initializer = biases)\n",
    "            dense = tf.contrib.layers.dropout(dense, keep_prob)\n",
    "    \n",
    "    # Make the predictions\n",
    "    with tf.name_scope('predictions'):\n",
    "        predictions = tf.contrib.layers.fully_connected(dense, \n",
    "                                                        num_outputs = 1, \n",
    "                                                        activation_fn=tf.sigmoid,\n",
    "                                                        weights_initializer = weights,\n",
    "                                                        biases_initializer = biases)\n",
    "        tf.summary.histogram('predictions', predictions)\n",
    "    \n",
    "    # Calculate the cost\n",
    "    with tf.name_scope('cost'):\n",
    "        cost = tf.losses.mean_squared_error(labels, predictions)\n",
    "        tf.summary.scalar('cost', cost)\n",
    "    \n",
    "    # Train the model\n",
    "    with tf.name_scope('train'):    \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "    # Determine the accuracy\n",
    "    with tf.name_scope('accuracy'):\n",
    "        correct_pred = tf.equal(tf.cast(tf.round(predictions), tf.int32), labels)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "        tf.summary.scalar('accuracy', accuracy)\n",
    "    \n",
    "    # Merge all of the summaries\n",
    "    merged = tf.summary.merge_all()    \n",
    "\n",
    "    # Export the nodes \n",
    "    export_nodes = ['inputs', 'labels', 'keep_prob', 'initial_state', 'final_state','accuracy',\n",
    "                    'predictions', 'cost', 'optimizer', 'merged']\n",
    "    Graph = namedtuple('Graph', export_nodes)\n",
    "    local_dict = locals()\n",
    "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
    "    \n",
    "    return graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(model, epochs, log_string):\n",
    "    '''Train the RNN'''\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # Used to determine when to stop the training early\n",
    "        valid_loss_summary = []\n",
    "        \n",
    "        # Keep track of which batch iteration is being trained\n",
    "        iteration = 0\n",
    "\n",
    "        print()\n",
    "        print(\"Training Model: {}\".format(log_string))\n",
    "\n",
    "        train_writer = tf.summary.FileWriter('./logs/3/train/{}'.format(log_string), sess.graph)\n",
    "        valid_writer = tf.summary.FileWriter('./logs/3/valid/{}'.format(log_string))\n",
    "\n",
    "        for e in range(epochs):\n",
    "            state = sess.run(model.initial_state)\n",
    "            \n",
    "            # Record progress with each epoch\n",
    "            train_loss = []\n",
    "            train_acc = []\n",
    "            val_acc = []\n",
    "            val_loss = []\n",
    "\n",
    "            with tqdm(total=len(x_train)) as pbar:\n",
    "                for _, (x, y) in enumerate(get_batches(x_train, y_train, batch_size), 1):\n",
    "                    feed = {model.inputs: x,\n",
    "                            model.labels: y[:, None],\n",
    "                            model.keep_prob: dropout,\n",
    "                            model.initial_state: state}\n",
    "                    summary, loss, acc, state, _ = sess.run([model.merged, \n",
    "                                                             model.cost, \n",
    "                                                             model.accuracy, \n",
    "                                                             model.final_state, \n",
    "                                                             model.optimizer], \n",
    "                                                            feed_dict=feed)                \n",
    "                    \n",
    "                    # Record the loss and accuracy of each training batch\n",
    "                    train_loss.append(loss)\n",
    "                    train_acc.append(acc)\n",
    "                    \n",
    "                    # Record the progress of training\n",
    "                    train_writer.add_summary(summary, iteration)\n",
    "                    \n",
    "                    iteration += 1\n",
    "                    pbar.update(batch_size)\n",
    "            \n",
    "            # Average the training loss and accuracy of each epoch\n",
    "            avg_train_loss = np.mean(train_loss)\n",
    "            avg_train_acc = np.mean(train_acc) \n",
    "\n",
    "            val_state = sess.run(model.initial_state)\n",
    "            with tqdm(total=len(x_valid)) as pbar:\n",
    "                for x, y in get_batches(x_valid, y_valid, batch_size):\n",
    "                    feed = {model.inputs: x,\n",
    "                            model.labels: y[:, None],\n",
    "                            model.keep_prob: 1,\n",
    "                            model.initial_state: val_state}\n",
    "                    summary, batch_loss, batch_acc, val_state = sess.run([model.merged, \n",
    "                                                                          model.cost, \n",
    "                                                                          model.accuracy, \n",
    "                                                                          model.final_state], \n",
    "                                                                         feed_dict=feed)\n",
    "                    \n",
    "                    # Record the validation loss and accuracy of each epoch\n",
    "                    val_loss.append(batch_loss)\n",
    "                    val_acc.append(batch_acc)\n",
    "                    pbar.update(batch_size)\n",
    "            \n",
    "            # Average the validation loss and accuracy of each epoch\n",
    "            avg_valid_loss = np.mean(val_loss)    \n",
    "            avg_valid_acc = np.mean(val_acc)\n",
    "            valid_loss_summary.append(avg_valid_loss)\n",
    "            \n",
    "            # Record the validation data's progress\n",
    "            valid_writer.add_summary(summary, iteration)\n",
    "\n",
    "            # Print the progress of each epoch\n",
    "            print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                  \"Train Loss: {:.3f}\".format(avg_train_loss),\n",
    "                  \"Train Acc: {:.3f}\".format(avg_train_acc),\n",
    "                  \"Valid Loss: {:.3f}\".format(avg_valid_loss),\n",
    "                  \"Valid Acc: {:.3f}\".format(avg_valid_acc))\n",
    "\n",
    "            # Stop training if the validation loss does not decrease after 3 epochs\n",
    "            if avg_valid_loss > min(valid_loss_summary):\n",
    "                print(\"No Improvement.\")\n",
    "                stop_early += 1\n",
    "                if stop_early == 3:\n",
    "                    break   \n",
    "            \n",
    "            # Reset stop_early if the validation loss finds a new low\n",
    "            # Save a checkpoint of the model\n",
    "            else:\n",
    "                print(\"New Record!\")\n",
    "                stop_early = 0\n",
    "                checkpoint = \"/Users/Dave/Desktop/Programming/Personal Projects/Movie_Reviews_Kaggle/sentiment_{}.ckpt\".format(log_string)\n",
    "                saver.save(sess, checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The default parameters of the model\n",
    "n_words = len(word_index)\n",
    "embed_size = 300\n",
    "batch_size = 250\n",
    "lstm_size = 128\n",
    "num_layers = 2\n",
    "dropout = 0.5\n",
    "learning_rate = 0.001\n",
    "epochs = 100\n",
    "multiple_fc = False\n",
    "fc_units = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Model: ru=64,fcl=True,fcu=128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [04:06<00:00, 104.23it/s]\n",
      "100%|██████████| 3750/3750 [00:11<00:00, 320.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/100 Train Loss: 0.241 Train Acc: 0.581 Valid Loss: 0.180 Valid Acc: 0.744\n",
      "New Record!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [03:54<00:00, 83.60it/s]\n",
      "100%|██████████| 3750/3750 [00:13<00:00, 290.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100 Train Loss: 0.140 Train Acc: 0.817 Valid Loss: 0.133 Valid Acc: 0.821\n",
      "New Record!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [03:57<00:00, 87.32it/s]\n",
      "100%|██████████| 3750/3750 [00:12<00:00, 320.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/100 Train Loss: 0.086 Train Acc: 0.893 Valid Loss: 0.124 Valid Acc: 0.848\n",
      "New Record!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [04:00<00:00, 88.02it/s]\n",
      "100%|██████████| 3750/3750 [00:12<00:00, 310.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/100 Train Loss: 0.062 Train Acc: 0.925 Valid Loss: 0.117 Valid Acc: 0.852\n",
      "New Record!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [03:28<00:00, 116.78it/s]\n",
      "100%|██████████| 3750/3750 [00:10<00:00, 352.03it/s]\n",
      "  0%|          | 0/21250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/100 Train Loss: 0.049 Train Acc: 0.943 Valid Loss: 0.127 Valid Acc: 0.852\n",
      "No Improvement.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [03:02<00:00, 116.16it/s]\n",
      "100%|██████████| 3750/3750 [00:10<00:00, 348.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/100 Train Loss: 0.042 Train Acc: 0.952 Valid Loss: 0.129 Valid Acc: 0.854\n",
      "New Record!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [03:02<00:00, 117.03it/s]\n",
      "100%|██████████| 3750/3750 [00:10<00:00, 346.53it/s]\n",
      "  0%|          | 0/21250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/100 Train Loss: 0.037 Train Acc: 0.958 Valid Loss: 0.128 Valid Acc: 0.851\n",
      "No Improvement.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [03:02<00:00, 116.98it/s]\n",
      "100%|██████████| 3750/3750 [00:10<00:00, 348.72it/s]\n",
      "  0%|          | 0/21250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/100 Train Loss: 0.048 Train Acc: 0.944 Valid Loss: 0.127 Valid Acc: 0.844\n",
      "No Improvement.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [03:02<00:00, 115.82it/s]\n",
      "100%|██████████| 3750/3750 [00:10<00:00, 350.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/100 Train Loss: 0.035 Train Acc: 0.960 Valid Loss: 0.127 Valid Acc: 0.854\n",
      "New Record!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [03:02<00:00, 116.63it/s]\n",
      "100%|██████████| 3750/3750 [00:10<00:00, 348.62it/s]\n",
      "  0%|          | 0/21250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/100 Train Loss: 0.024 Train Acc: 0.974 Valid Loss: 0.124 Valid Acc: 0.854\n",
      "No Improvement.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [03:02<00:00, 117.27it/s]\n",
      "100%|██████████| 3750/3750 [00:10<00:00, 349.94it/s]\n",
      "  0%|          | 0/21250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/100 Train Loss: 0.021 Train Acc: 0.978 Valid Loss: 0.133 Valid Acc: 0.844\n",
      "No Improvement.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [03:01<00:00, 115.85it/s]\n",
      "100%|██████████| 3750/3750 [00:10<00:00, 354.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/100 Train Loss: 0.020 Train Acc: 0.979 Valid Loss: 0.140 Valid Acc: 0.841\n",
      "No Improvement.\n",
      "\n",
      "Training Model: ru=64,fcl=True,fcu=256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [03:02<00:00, 116.65it/s]\n",
      "100%|██████████| 3750/3750 [00:10<00:00, 343.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/100 Train Loss: 0.235 Train Acc: 0.621 Valid Loss: 0.172 Valid Acc: 0.752\n",
      "New Record!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [03:02<00:00, 116.79it/s]\n",
      "100%|██████████| 3750/3750 [00:10<00:00, 349.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100 Train Loss: 0.136 Train Acc: 0.818 Valid Loss: 0.121 Valid Acc: 0.837\n",
      "New Record!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [03:02<00:00, 117.41it/s]\n",
      "100%|██████████| 3750/3750 [00:11<00:00, 339.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/100 Train Loss: 0.086 Train Acc: 0.891 Valid Loss: 0.111 Valid Acc: 0.852\n",
      "New Record!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [03:02<00:00, 117.37it/s]\n",
      "100%|██████████| 3750/3750 [00:11<00:00, 334.51it/s]\n",
      "  0%|          | 0/21250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/100 Train Loss: 0.066 Train Acc: 0.918 Valid Loss: 0.131 Valid Acc: 0.836\n",
      "No Improvement.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [03:03<00:00, 116.78it/s]\n",
      "100%|██████████| 3750/3750 [00:10<00:00, 346.84it/s]\n",
      "  0%|          | 0/21250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/100 Train Loss: 0.055 Train Acc: 0.933 Valid Loss: 0.139 Valid Acc: 0.831\n",
      "No Improvement.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [03:02<00:00, 116.88it/s]\n",
      "100%|██████████| 3750/3750 [00:10<00:00, 347.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/100 Train Loss: 0.043 Train Acc: 0.949 Valid Loss: 0.128 Valid Acc: 0.845\n",
      "No Improvement.\n",
      "\n",
      "Training Model: ru=64,fcl=False,fcu=128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [03:02<00:00, 116.27it/s]\n",
      "100%|██████████| 3750/3750 [00:10<00:00, 344.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/100 Train Loss: 0.213 Train Acc: 0.662 Valid Loss: 0.165 Valid Acc: 0.761\n",
      "New Record!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [03:02<00:00, 115.83it/s]\n",
      "100%|██████████| 3750/3750 [00:10<00:00, 347.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100 Train Loss: 0.134 Train Acc: 0.823 Valid Loss: 0.133 Valid Acc: 0.811\n",
      "New Record!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [03:01<00:00, 117.67it/s]\n",
      "100%|██████████| 3750/3750 [00:10<00:00, 354.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/100 Train Loss: 0.092 Train Acc: 0.885 Valid Loss: 0.130 Valid Acc: 0.822\n",
      "New Record!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [03:02<00:00, 116.25it/s]\n",
      "100%|██████████| 3750/3750 [00:10<00:00, 351.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/100 Train Loss: 0.067 Train Acc: 0.919 Valid Loss: 0.118 Valid Acc: 0.846\n",
      "New Record!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [03:02<00:00, 115.59it/s]\n",
      "100%|██████████| 3750/3750 [00:10<00:00, 349.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/100 Train Loss: 0.056 Train Acc: 0.934 Valid Loss: 0.120 Valid Acc: 0.848\n",
      "New Record!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [03:02<00:00, 116.70it/s]\n",
      "100%|██████████| 3750/3750 [00:10<00:00, 333.09it/s]\n",
      "  0%|          | 0/21250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/100 Train Loss: 0.058 Train Acc: 0.930 Valid Loss: 0.127 Valid Acc: 0.844\n",
      "No Improvement.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [03:02<00:00, 116.56it/s]\n",
      "100%|██████████| 3750/3750 [00:10<00:00, 348.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/100 Train Loss: 0.051 Train Acc: 0.938 Valid Loss: 0.111 Valid Acc: 0.859\n",
      "New Record!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [03:02<00:00, 117.39it/s]\n",
      "100%|██████████| 3750/3750 [00:10<00:00, 347.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/100 Train Loss: 0.030 Train Acc: 0.966 Valid Loss: 0.114 Valid Acc: 0.865\n",
      "New Record!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [03:02<00:00, 116.32it/s]\n",
      "100%|██████████| 3750/3750 [00:10<00:00, 349.11it/s]\n",
      "  0%|          | 0/21250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/100 Train Loss: 0.024 Train Acc: 0.974 Valid Loss: 0.124 Valid Acc: 0.857\n",
      "No Improvement.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [03:02<00:00, 116.94it/s]\n",
      "100%|██████████| 3750/3750 [00:10<00:00, 333.52it/s]\n",
      "  0%|          | 0/21250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/100 Train Loss: 0.022 Train Acc: 0.976 Valid Loss: 0.120 Valid Acc: 0.863\n",
      "No Improvement.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [03:02<00:00, 116.10it/s]\n",
      "100%|██████████| 3750/3750 [00:10<00:00, 347.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/100 Train Loss: 0.022 Train Acc: 0.976 Valid Loss: 0.115 Valid Acc: 0.868\n",
      "New Record!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [03:02<00:00, 116.40it/s]\n",
      "100%|██████████| 3750/3750 [00:10<00:00, 343.92it/s]\n",
      "  0%|          | 0/21250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/100 Train Loss: 0.018 Train Acc: 0.981 Valid Loss: 0.125 Valid Acc: 0.857\n",
      "No Improvement.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [03:02<00:00, 116.39it/s]\n",
      "100%|██████████| 3750/3750 [00:10<00:00, 346.49it/s]\n",
      "  0%|          | 0/21250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12/100 Train Loss: 0.016 Train Acc: 0.983 Valid Loss: 0.127 Valid Acc: 0.852\n",
      "No Improvement.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [03:02<00:00, 116.41it/s]\n",
      "100%|██████████| 3750/3750 [00:10<00:00, 351.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13/100 Train Loss: 0.016 Train Acc: 0.983 Valid Loss: 0.128 Valid Acc: 0.856\n",
      "No Improvement.\n",
      "\n",
      "Training Model: ru=64,fcl=False,fcu=256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [03:02<00:00, 117.40it/s]\n",
      "100%|██████████| 3750/3750 [00:10<00:00, 350.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/100 Train Loss: 0.231 Train Acc: 0.633 Valid Loss: 0.151 Valid Acc: 0.786\n",
      "New Record!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [03:02<00:00, 115.96it/s]\n",
      "100%|██████████| 3750/3750 [00:10<00:00, 352.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100 Train Loss: 0.129 Train Acc: 0.827 Valid Loss: 0.137 Valid Acc: 0.816\n",
      "New Record!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [03:03<00:00, 115.60it/s]\n",
      "100%|██████████| 3750/3750 [00:10<00:00, 347.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/100 Train Loss: 0.086 Train Acc: 0.889 Valid Loss: 0.114 Valid Acc: 0.860\n",
      "New Record!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [03:02<00:00, 115.76it/s]\n",
      "100%|██████████| 3750/3750 [00:10<00:00, 352.93it/s]\n",
      "  0%|          | 0/21250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/100 Train Loss: 0.065 Train Acc: 0.919 Valid Loss: 0.132 Valid Acc: 0.839\n",
      "No Improvement.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [03:02<00:00, 116.80it/s]\n",
      "100%|██████████| 3750/3750 [00:10<00:00, 338.22it/s]\n",
      "  0%|          | 0/21250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/100 Train Loss: 0.055 Train Acc: 0.934 Valid Loss: 0.117 Valid Acc: 0.855\n",
      "No Improvement.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [03:03<00:00, 114.87it/s]\n",
      "100%|██████████| 3750/3750 [00:10<00:00, 346.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/100 Train Loss: 0.043 Train Acc: 0.948 Valid Loss: 0.119 Valid Acc: 0.856\n",
      "No Improvement.\n",
      "\n",
      "Training Model: ru=128,fcl=True,fcu=128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [05:40<00:00, 62.64it/s]\n",
      "100%|██████████| 3750/3750 [00:20<00:00, 182.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/100 Train Loss: 0.227 Train Acc: 0.633 Valid Loss: 0.162 Valid Acc: 0.768\n",
      "New Record!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [05:39<00:00, 62.70it/s]\n",
      "100%|██████████| 3750/3750 [00:20<00:00, 183.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100 Train Loss: 0.132 Train Acc: 0.827 Valid Loss: 0.132 Valid Acc: 0.827\n",
      "New Record!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [05:39<00:00, 62.88it/s]\n",
      "100%|██████████| 3750/3750 [00:20<00:00, 184.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/100 Train Loss: 0.090 Train Acc: 0.888 Valid Loss: 0.124 Valid Acc: 0.840\n",
      "New Record!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [05:40<00:00, 62.61it/s]\n",
      "100%|██████████| 3750/3750 [00:20<00:00, 182.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/100 Train Loss: 0.069 Train Acc: 0.918 Valid Loss: 0.124 Valid Acc: 0.849\n",
      "New Record!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [05:39<00:00, 62.56it/s]\n",
      "100%|██████████| 3750/3750 [00:20<00:00, 182.46it/s]\n",
      "  0%|          | 0/21250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/100 Train Loss: 0.059 Train Acc: 0.930 Valid Loss: 0.134 Valid Acc: 0.843\n",
      "No Improvement.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [05:38<00:00, 62.54it/s]\n",
      "100%|██████████| 3750/3750 [00:20<00:00, 181.83it/s]\n",
      "  0%|          | 0/21250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/100 Train Loss: 0.051 Train Acc: 0.941 Valid Loss: 0.130 Valid Acc: 0.848\n",
      "No Improvement.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [05:39<00:00, 62.57it/s]\n",
      "100%|██████████| 3750/3750 [00:20<00:00, 183.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/100 Train Loss: 0.052 Train Acc: 0.938 Valid Loss: 0.142 Valid Acc: 0.819\n",
      "No Improvement.\n",
      "\n",
      "Training Model: ru=128,fcl=True,fcu=256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [05:40<00:00, 62.53it/s]\n",
      "100%|██████████| 3750/3750 [00:20<00:00, 184.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/100 Train Loss: 0.218 Train Acc: 0.662 Valid Loss: 0.162 Valid Acc: 0.778\n",
      "New Record!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [05:40<00:00, 62.82it/s]\n",
      "100%|██████████| 3750/3750 [00:20<00:00, 183.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100 Train Loss: 0.131 Train Acc: 0.829 Valid Loss: 0.121 Valid Acc: 0.837\n",
      "New Record!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [05:39<00:00, 62.58it/s]\n",
      "100%|██████████| 3750/3750 [00:20<00:00, 184.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/100 Train Loss: 0.088 Train Acc: 0.889 Valid Loss: 0.111 Valid Acc: 0.854\n",
      "New Record!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [05:39<00:00, 62.54it/s]\n",
      "100%|██████████| 3750/3750 [00:20<00:00, 182.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/100 Train Loss: 0.063 Train Acc: 0.924 Valid Loss: 0.111 Valid Acc: 0.856\n",
      "New Record!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [05:39<00:00, 63.01it/s]\n",
      "100%|██████████| 3750/3750 [00:20<00:00, 182.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/100 Train Loss: 0.049 Train Acc: 0.943 Valid Loss: 0.117 Valid Acc: 0.858\n",
      "New Record!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [05:38<00:00, 63.14it/s]\n",
      "100%|██████████| 3750/3750 [00:20<00:00, 184.13it/s]\n",
      "  0%|          | 0/21250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/100 Train Loss: 0.044 Train Acc: 0.949 Valid Loss: 0.127 Valid Acc: 0.854\n",
      "No Improvement.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [05:39<00:00, 62.59it/s]\n",
      "100%|██████████| 3750/3750 [00:20<00:00, 182.41it/s]\n",
      "  0%|          | 0/21250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/100 Train Loss: 0.039 Train Acc: 0.954 Valid Loss: 0.124 Valid Acc: 0.852\n",
      "No Improvement.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [05:39<00:00, 63.05it/s]\n",
      "100%|██████████| 3750/3750 [00:20<00:00, 183.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/100 Train Loss: 0.060 Train Acc: 0.928 Valid Loss: 0.126 Valid Acc: 0.847\n",
      "No Improvement.\n",
      "\n",
      "Training Model: ru=128,fcl=False,fcu=128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [05:39<00:00, 62.74it/s]\n",
      "100%|██████████| 3750/3750 [00:20<00:00, 182.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/100 Train Loss: 0.196 Train Acc: 0.699 Valid Loss: 0.137 Valid Acc: 0.810\n",
      "New Record!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [05:38<00:00, 62.90it/s]\n",
      "100%|██████████| 3750/3750 [00:20<00:00, 183.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100 Train Loss: 0.110 Train Acc: 0.855 Valid Loss: 0.113 Valid Acc: 0.847\n",
      "New Record!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [05:38<00:00, 63.04it/s]\n",
      "100%|██████████| 3750/3750 [00:20<00:00, 182.96it/s]\n",
      "  0%|          | 0/21250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/100 Train Loss: 0.075 Train Acc: 0.905 Valid Loss: 0.124 Valid Acc: 0.833\n",
      "No Improvement.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [05:38<00:00, 62.78it/s]\n",
      "100%|██████████| 3750/3750 [00:20<00:00, 183.99it/s]\n",
      "  0%|          | 0/21250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/100 Train Loss: 0.057 Train Acc: 0.932 Valid Loss: 0.142 Valid Acc: 0.837\n",
      "No Improvement.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [05:37<00:00, 62.54it/s]\n",
      "100%|██████████| 3750/3750 [00:20<00:00, 182.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/100 Train Loss: 0.054 Train Acc: 0.936 Valid Loss: 0.123 Valid Acc: 0.851\n",
      "New Record!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [05:39<00:00, 62.82it/s]\n",
      "100%|██████████| 3750/3750 [00:20<00:00, 181.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/100 Train Loss: 0.039 Train Acc: 0.955 Valid Loss: 0.116 Valid Acc: 0.860\n",
      "New Record!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [05:38<00:00, 63.06it/s]\n",
      "100%|██████████| 3750/3750 [00:20<00:00, 179.94it/s]\n",
      "  0%|          | 0/21250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/100 Train Loss: 0.034 Train Acc: 0.962 Valid Loss: 0.122 Valid Acc: 0.853\n",
      "No Improvement.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [05:38<00:00, 63.10it/s]\n",
      "100%|██████████| 3750/3750 [00:20<00:00, 181.67it/s]\n",
      "  0%|          | 0/21250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/100 Train Loss: 0.030 Train Acc: 0.966 Valid Loss: 0.125 Valid Acc: 0.847\n",
      "No Improvement.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [05:38<00:00, 62.92it/s]\n",
      "100%|██████████| 3750/3750 [00:20<00:00, 182.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/100 Train Loss: 0.024 Train Acc: 0.973 Valid Loss: 0.122 Valid Acc: 0.861\n",
      "New Record!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [05:38<00:00, 62.27it/s]\n",
      "100%|██████████| 3750/3750 [00:20<00:00, 183.37it/s]\n",
      "  0%|          | 0/21250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/100 Train Loss: 0.021 Train Acc: 0.977 Valid Loss: 0.123 Valid Acc: 0.855\n",
      "No Improvement.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [05:37<00:00, 62.84it/s]\n",
      "100%|██████████| 3750/3750 [00:20<00:00, 182.57it/s]\n",
      "  0%|          | 0/21250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/100 Train Loss: 0.022 Train Acc: 0.976 Valid Loss: 0.123 Valid Acc: 0.857\n",
      "No Improvement.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [05:38<00:00, 62.83it/s]\n",
      "100%|██████████| 3750/3750 [00:20<00:00, 182.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/100 Train Loss: 0.023 Train Acc: 0.974 Valid Loss: 0.139 Valid Acc: 0.839\n",
      "No Improvement.\n",
      "\n",
      "Training Model: ru=128,fcl=False,fcu=256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [05:39<00:00, 62.96it/s]\n",
      "100%|██████████| 3750/3750 [00:20<00:00, 182.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/100 Train Loss: 0.201 Train Acc: 0.697 Valid Loss: 0.134 Valid Acc: 0.810\n",
      "New Record!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [05:39<00:00, 62.76it/s]\n",
      "100%|██████████| 3750/3750 [00:20<00:00, 183.03it/s]\n",
      "  0%|          | 0/21250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100 Train Loss: 0.125 Train Acc: 0.835 Valid Loss: 0.143 Valid Acc: 0.798\n",
      "No Improvement.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [05:38<00:00, 62.86it/s]\n",
      "100%|██████████| 3750/3750 [00:20<00:00, 183.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/100 Train Loss: 0.093 Train Acc: 0.885 Valid Loss: 0.110 Valid Acc: 0.854\n",
      "New Record!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [05:39<00:00, 62.73it/s]\n",
      "100%|██████████| 3750/3750 [00:20<00:00, 183.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/100 Train Loss: 0.060 Train Acc: 0.928 Valid Loss: 0.106 Valid Acc: 0.863\n",
      "New Record!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [05:39<00:00, 62.38it/s]\n",
      "100%|██████████| 3750/3750 [00:20<00:00, 184.73it/s]\n",
      "  0%|          | 0/21250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/100 Train Loss: 0.047 Train Acc: 0.946 Valid Loss: 0.118 Valid Acc: 0.857\n",
      "No Improvement.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [05:39<00:00, 62.73it/s]\n",
      "100%|██████████| 3750/3750 [00:20<00:00, 182.26it/s]\n",
      "  0%|          | 0/21250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/100 Train Loss: 0.043 Train Acc: 0.950 Valid Loss: 0.120 Valid Acc: 0.859\n",
      "No Improvement.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [05:39<00:00, 62.80it/s]\n",
      "100%|██████████| 3750/3750 [00:20<00:00, 182.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/100 Train Loss: 0.042 Train Acc: 0.951 Valid Loss: 0.120 Valid Acc: 0.861\n",
      "No Improvement.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model with the desired tuning parameters\n",
    "for lstm_size in [64,128]:\n",
    "    for multiple_fc in [True, False]:\n",
    "        for fc_units in [128, 256]:\n",
    "            log_string = 'ru={},fcl={},fcu={}'.format(lstm_size,\n",
    "                                                      multiple_fc,\n",
    "                                                      fc_units)\n",
    "            model = build_rnn(n_words = n_words, \n",
    "                              embed_size = embed_size,\n",
    "                              batch_size = batch_size,\n",
    "                              lstm_size = lstm_size,\n",
    "                              num_layers = num_layers,\n",
    "                              dropout = dropout,\n",
    "                              learning_rate = learning_rate,\n",
    "                              multiple_fc = multiple_fc,\n",
    "                              fc_units = fc_units)            \n",
    "            train(model, epochs, log_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make the Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_predictions(lstm_size, multiple_fc, fc_units, checkpoint):\n",
    "    '''Predict the sentiment of the testing data'''\n",
    "    \n",
    "    # Record all of the predictions\n",
    "    all_preds = []\n",
    "\n",
    "    model = build_rnn(n_words = n_words, \n",
    "                      embed_size = embed_size,\n",
    "                      batch_size = batch_size,\n",
    "                      lstm_size = lstm_size,\n",
    "                      num_layers = num_layers,\n",
    "                      dropout = dropout,\n",
    "                      learning_rate = learning_rate,\n",
    "                      multiple_fc = multiple_fc,\n",
    "                      fc_units = fc_units) \n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        saver = tf.train.Saver()\n",
    "        # Load the model\n",
    "        saver.restore(sess, checkpoint)\n",
    "        test_state = sess.run(model.initial_state)\n",
    "        for _, x in enumerate(get_test_batches(x_test, batch_size), 1):\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1,\n",
    "                    model.initial_state: test_state}\n",
    "            predictions = sess.run(model.predictions, feed_dict=feed)\n",
    "            for pred in predictions:\n",
    "                all_preds.append(float(pred))\n",
    "                \n",
    "    return all_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to compare the results of the best three models, based on the validation data. Then average the predictions of these three models, which should produce an even better set of predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "checkpoint1 = \"/Users/Dave/Desktop/Programming/Personal Projects/Movie_Reviews_Kaggle/sentiment_ru=128,fcl=False,fcu=256.ckpt\"\n",
    "checkpoint2 = \"/Users/Dave/Desktop/Programming/Personal Projects/Movie_Reviews_Kaggle/sentiment_ru=128,fcl=False,fcu=128.ckpt\"\n",
    "checkpoint3 = \"/Users/Dave/Desktop/Programming/Personal Projects/Movie_Reviews_Kaggle/sentiment_ru=64,fcl=True,fcu=256.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make predictions using the best 3 models\n",
    "predictions1 = make_predictions(128, False, 256, checkpoint1)\n",
    "predictions2 = make_predictions(128, False, 128, checkpoint2)\n",
    "predictions3 = make_predictions(64, True, 256, checkpoint3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Average the best three predictions\n",
    "predictions_combined = (pd.DataFrame(predictions1) + pd.DataFrame(predictions2) + pd.DataFrame(predictions3))/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_submission(predictions, string):\n",
    "    '''write the predictions to a csv file'''\n",
    "    submission = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":predictions})\n",
    "    submission.to_csv(\"submission_{}.csv\".format(string), index=False, quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write_submission(predictions1, \"ru=128,fcl=False,fcu=256\") \n",
    "write_submission(predictions2, \"ru=128,fcl=False,fcu=128\") \n",
    "write_submission(predictions3, \"ru=64,fcl=True,fcu=256\") \n",
    "write_submission(predictions_combined.ix[:,0], \"combined\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The results of the predictions are as follows (Kaggle used area under the ROC curve to evaluation submissions):\n",
    "- Predictions1: 0.919\n",
    "- Predictions2: 0.914\n",
    "- Predictions3: 0.916\n",
    "- Combined Predictions: 0.935"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am rather pleased by how this analysis has finished. Now I am much more confident in using TensorBoard to improve the design of a model and I have achieved rather good results. The combined predictions' submission ranks 206 out of 578, top 35.6%. This result could have been improved by using a larger model, using pretrained vectors (such as GloVe), and using an ensemble of more predictions. Although it would be nice to carry out these efforts and improve my results, I feel that would not be the best use of my time. There are more complicated projects that I would like to work on now, rather than training this model multiple times for a competition that has already concluded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
